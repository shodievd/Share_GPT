{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0d74bb",
   "metadata": {
    "id": "ab0d74bb"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import binom, poisson, norm\n",
    "import statsmodels.api as sm\n",
    "\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "%config InlineBackend.figure_format = 'retina'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8w0N76NwsLy5",
   "metadata": {
    "id": "8w0N76NwsLy5"
   },
   "outputs": [],
   "source": [
    "!wget https://www.wolframcloud.com/objects/282a8450-4101-41cc-aded-2edd3ab69e4d --output-document Prussian-Horse-Kick-Data.csv\n",
    "!wget https://raw.githubusercontent.com/niquepolice/misc-files/master/StudentData.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Rf6-HZKzEvF-",
   "metadata": {
    "id": "Rf6-HZKzEvF-"
   },
   "source": [
    "# Please, enter your full name and telegram username here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93228344",
   "metadata": {
    "id": "93228344"
   },
   "source": [
    "# Poisson regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257e8fb2",
   "metadata": {
    "id": "257e8fb2"
   },
   "outputs": [],
   "source": [
    "# https://datarepository.wolframcloud.com/resources/Prussian-Horse-Kick-Data\n",
    "# The data give the number of soldiers in the Prussian cavalry killed by horse kicks, by corp membership and by year.\n",
    "\n",
    "df = pd.read_csv(\"Prussian-Horse-Kick-Data.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9b83ae",
   "metadata": {
    "id": "ae9b83ae"
   },
   "outputs": [],
   "source": [
    "series = df.iloc[:, 1:].values.flatten() \n",
    "plt.hist(series, bins=100, label=\"data\")\n",
    "\n",
    "x = np.arange(5)\n",
    "plt.scatter(x, poisson.pmf(x, series.mean()) * series.size, color='m', label=\"poisson\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014cd60b",
   "metadata": {
    "id": "014cd60b"
   },
   "source": [
    "# Predicting number of students awards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70715820",
   "metadata": {
    "id": "70715820"
   },
   "outputs": [],
   "source": [
    "#  https://search.r-project.org/CRAN/refmans/gorica/html/academic_awards.html\n",
    "df = pd.read_csv(\"StudentData.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2847e2",
   "metadata": {
    "id": "fd2847e2"
   },
   "outputs": [],
   "source": [
    "df[df.prog == 2].num_awards.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ade3ccd",
   "metadata": {
    "id": "0ade3ccd"
   },
   "outputs": [],
   "source": [
    "df_dummy = pd.get_dummies(df, columns=[\"prog\"])\n",
    "df_dummy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b341a46",
   "metadata": {
    "id": "6b341a46"
   },
   "source": [
    "## Simply use statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fa2db9",
   "metadata": {
    "id": "25fa2db9"
   },
   "outputs": [],
   "source": [
    "Y = df_dummy[\"num_awards\"].values\n",
    "X = sm.tools.add_constant(df_dummy[[\"math\", \"prog_1\", \"prog_2\", \"prog_3\"]].values)\n",
    "fit_res = sm.Poisson(Y, X).fit()\n",
    "fit_res.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e63730",
   "metadata": {
    "id": "65e63730"
   },
   "outputs": [],
   "source": [
    "sm_pred = fit_res.predict(X)\n",
    "\n",
    "def plot(*preds):\n",
    "    plt.figure(figsize=(15, 4))\n",
    "    ax = None\n",
    "    for prog in [1, 2, 3]:\n",
    "        ax = plt.subplot(130 + prog, sharey=ax)\n",
    "        inds = df.prog == prog \n",
    "        plt.scatter(df.math[inds], Y[inds], label='y')\n",
    "        for i, pred in enumerate(preds):\n",
    "            plt.scatter(df.math[inds], pred[inds], label=f'prediction {i + 1}')\n",
    "        plt.title(f\"prog={prog}\")\n",
    "        plt.legend()\n",
    "        \n",
    "plot(sm_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73569d65",
   "metadata": {
    "id": "73569d65"
   },
   "source": [
    "### 1. Exponential family\n",
    "$$p(y, \\lambda) = \\frac{e^{-\\lambda} \\lambda^y}{y!}$$\n",
    "\n",
    "$$b(y) exp\\left(\\eta^T T(y) - a(\\eta)\\right)$$ \n",
    "    \n",
    "<!-- <font color='red'>Show that the Poisson distribution is in the exponential family.</font> -->\n",
    "Poisson distribution is in the exponential family:\n",
    "$$\\frac{1}{y!} \\exp(-y + y \\ln \\lambda) = \\frac{1}{y!} \\exp(\\eta y - e^\\eta)$$\n",
    "\n",
    "The connection between $\\eta$ and $\\lambda$ is  $\\ln \\lambda = \\eta$ \n",
    "\n",
    "<!-- The connection between $\\eta$ and $\\lambda$ is:  $\\color{red}{?}$  -->\n",
    "\n",
    "### 2. Decision function \n",
    "$$h_\\theta(x) = \\mathbb E[y | x] = \\lambda = e^{\\eta} = e^{\\theta^\\top x}$$\n",
    "\n",
    "### 3. Objective function / maximize log likelihood \n",
    "The loss function for the Poisson regression is\n",
    "$$f(\\theta) = \\sum_k \\left(e^{x_k^\\top \\theta} - y_k x_k^\\top \\theta \\right)$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56da1d2f",
   "metadata": {
    "id": "56da1d2f"
   },
   "source": [
    "# Problem 1. (1.5) Gradient\n",
    "\n",
    "Denoting $X = \\begin{pmatrix}x_1^\\top \\\\ \\vdots \\\\ x_n^\\top \\end{pmatrix}$  the matrix, which rows is our inputs $x_k$ (this is how a bunch of vectors is usually represented in `numpy`), and $Y=\\begin{pmatrix}y_1 \\\\ \\vdots \\\\ y_n \\end{pmatrix}$ the vector of outputs, we can rewrite $f$ in the matrix form (so that $f$ could be calculated efficiently with `numpy`'s backend without in-python loops)\n",
    "$$f(\\theta) = \\mathbf 1^\\top \\left(\\exp(X\\theta) - Y \\odot X \\theta \\right),$$\n",
    "there $\\odot$ denotes componentwise multiplication, and $\\exp$ is also elementwise.\n",
    "\n",
    "- Find the gradient of the loss $\\nabla f(\\theta)$ and write it **in the matrix form** in the cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f01a19",
   "metadata": {
    "id": "30f01a19"
   },
   "source": [
    "# Solution\n",
    "\n",
    "here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ba3fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and here \n",
    "def f(theta: np.ndarray):\n",
    "    return (np.exp(X @ theta) - Y * (X @ theta)).sum()\n",
    "\n",
    "    \n",
    "def gradf(theta: np.ndarray):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b884a449",
   "metadata": {
    "id": "b884a449"
   },
   "source": [
    "# Problem 2. (1) Gradient Descent\n",
    "Probably, the simpliest optimization method is the gradient descent with the constant step-size $h$:\n",
    "$$\\theta^{i+1} = \\theta^i - h \\nabla f(\\theta^i).$$\n",
    "\n",
    "- Implement this algorithm, using the draft below.\n",
    "\n",
    "- If staring point $\\theta^{0}$ is set to $\\vec 0$, what is the maximum step-size value $h$, with which the method still could be called a \"descent\", i.e. $f(\\theta^i)$ monotonously decreases? Use f\"{h:.0e}\" format for the answer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a26ff3",
   "metadata": {
    "id": "74a26ff3"
   },
   "outputs": [],
   "source": [
    "dim = X.shape[1] \n",
    "theta = np.zeros(dim)\n",
    "\n",
    "niters = # ?\n",
    "h = # ?\n",
    "flog = []\n",
    "for i in range(niters):\n",
    "    flog.append(f(theta))\n",
    "    # antigradient step here\n",
    "    \n",
    "    \n",
    "plt.plot(flog)\n",
    "plt.title(\"Loss\")\n",
    "plt.xlabel(\"iter\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f09f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# maximum h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9d72f8",
   "metadata": {
    "id": "2d9d72f8"
   },
   "outputs": [],
   "source": [
    "def predict(x, theta):\n",
    "    pass  # decision function here\n",
    "\n",
    "plot(sm_pred, predict(X, theta))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfa3e7b",
   "metadata": {
    "id": "8bfa3e7b"
   },
   "source": [
    "# Problem 3. (1.5+1) Hessian\n",
    "- 3.1 (1.5) Find $\\nabla^2 f(\\theta)$ and write it in the matrix form. \n",
    "\n",
    "- 3.1 (1) If $Ker X = 0$, is it true that $\\nabla^2 f(\\theta) > 0$ ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10e2e41",
   "metadata": {
    "id": "b10e2e41"
   },
   "source": [
    "# Solution\n",
    "here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9868d7c5",
   "metadata": {
    "id": "d114db93"
   },
   "outputs": [],
   "source": [
    "# and here\n",
    "def hessf(theta: np.ndarray):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de1eba6",
   "metadata": {
    "id": "3de1eba6"
   },
   "source": [
    "# Problem 4. (1.5) Newton's method\n",
    "Another classical optimization method is the Newton's method:\n",
    "$$\\theta^{k+1} = \\theta^{k} - [\\nabla^2 f(\\theta^k)]^{-1} \\nabla f(\\theta^k)$$\n",
    "\n",
    "- Implement the method. \n",
    "\n",
    "- Describe its behaviour on our optimization problem. Try different starting points $\\theta^0$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d114db93",
   "metadata": {
    "id": "d114db93"
   },
   "outputs": [],
   "source": [
    "# solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e749bc3f",
   "metadata": {
    "id": "e749bc3f"
   },
   "source": [
    "and here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080dc76f",
   "metadata": {
    "id": "080dc76f"
   },
   "source": [
    "# Problem 5. (1) Damped Newton's method \n",
    "\n",
    "Damped Newton's method is a modification of the original one with introduced step-sizes $h^k > 0$:\n",
    "$$\\theta^{k+1} = \\theta^{k} - \\color{red}{h^k} [\\nabla^2 f(\\theta^k)]^{-1} \\nabla f(\\theta^k)$$\n",
    "\n",
    "Common-used strategy for choosing step-size is the Armijo rule. Armijo rule applies for the methods of the following form:\n",
    "$$\\theta^{k+1} = \\theta^k - h^k g^k.$$\n",
    "Armijo rule (backtracking linesearch) is the following algorithm:\n",
    "1. Set $h^k = h_0$\n",
    "2. While $f(\\theta^k - h^k g^k) > f(\\theta^k) - \\alpha h^k (g^k)^\\top \\nabla f(\\theta^k)$ do $h^k = h^k \\cdot \\rho$.\n",
    "\n",
    "Here $\\alpha \\geq 0$ and $0 < \\rho < 1$ are hyperparameters. Usually $\\alpha \\leq 0.3$.\n",
    "\n",
    "- Show that Armijo rule is correctly defined for damped Newton's method applied to smooth $f$, i.e. while-loop terminates with some $h^k > 0$\n",
    "\n",
    "- Implement damped Newton's method. How much worse (roughly) arithmetic complexity of its iteration comparing to classical Newton's method?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134cb4b1",
   "metadata": {
    "id": "134cb4b1"
   },
   "source": [
    "# Solution\n",
    "here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c944d5",
   "metadata": {
    "id": "20c944d5"
   },
   "outputs": [],
   "source": [
    "# and here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e790efd8",
   "metadata": {
    "id": "e790efd8"
   },
   "source": [
    "# Problem 6*. (3) Speedup\n",
    "\n",
    "- Suggest (it's possible you've done it already in problems 4-5) the way to recalculate $g^k$ with $O(n^2)$  arithmetic complexity (matrix $X \\in \\mathbb R^{O(n) \\times O(n)}$), meaning that the first iteration can have the complexity up to $O(n^3)$, and all the rest are $O(n^2)$.\n",
    "- Implement it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7631374c",
   "metadata": {
    "id": "134cb4b1"
   },
   "source": [
    "# Solution\n",
    "here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f101f4b8",
   "metadata": {
    "id": "20c944d5"
   },
   "outputs": [],
   "source": [
    "# and here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af5d9fb",
   "metadata": {
    "id": "a9e7ae4f"
   },
   "source": [
    "# Problem 7 (2) (offtop about GD). \n",
    "\n",
    "Упр 1.2 пособия: покажите, что если для минимизации положительно определённой квадратичной формы $f(x)=\\frac{1}{2}\\langle A x, x\\rangle-\\langle b, x\\rangle \\rightarrow \\min _{x \\in \\mathbb{R}^n}$ использовать градиентный спуск с $h^k=1 / \\lambda_{k+1}$ где $\\lambda_{k+1}-(k+1)$-е собственное значение матрицы $A\\left(0<\\mu=\\lambda_1 \\leqslant \\ldots \\leqslant \\lambda_n=L\\right)$, то независимо от точки старта метод будет конечен: $x^n=x_*$, где $A x_*=b$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a9e8a0",
   "metadata": {},
   "source": [
    "# Solution\n",
    "here "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c8e32c",
   "metadata": {},
   "source": [
    "# Problem 8 (2) (offtop about GD 2)\n",
    "\n",
    "Покажите, что в случае вырожденной квадратичной функции $f(x) = \\frac12 \\langle Ax, x\\rangle - \\langle b, x\\rangle, \\; \\mu = 0$ градиентный спуск линейно сходится ко множеству $\\{x: Ax - b \\in \\text{Ker } A\\}$. Сходится ли GD на такой задаче в обычном смысле этого слова? Вспомните о том, что когда функция не сильно выпукла на всей области определения, она всё ещё может быть сильно выпукла на некотором подмножестве/подпространстве."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3202a7b",
   "metadata": {},
   "source": [
    "# Solution \n",
    "here"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
